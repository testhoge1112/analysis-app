{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "215f226b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>å°ç•ª</th>\n",
       "      <th>Gæ•°</th>\n",
       "      <th>BB</th>\n",
       "      <th>RB</th>\n",
       "      <th>å·®æš</th>\n",
       "      <th>å‡ºç‡</th>\n",
       "      <th>åˆæˆ</th>\n",
       "      <th>BBç‡</th>\n",
       "      <th>RBç‡</th>\n",
       "      <th>RB_denominator</th>\n",
       "      <th>RB_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>555</td>\n",
       "      <td>552144</td>\n",
       "      <td>2056</td>\n",
       "      <td>1870</td>\n",
       "      <td>39191</td>\n",
       "      <td>102.4%</td>\n",
       "      <td>1/141</td>\n",
       "      <td>1/269</td>\n",
       "      <td>1/295</td>\n",
       "      <td>295.0</td>\n",
       "      <td>0.003390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>564</td>\n",
       "      <td>444413</td>\n",
       "      <td>1704</td>\n",
       "      <td>1401</td>\n",
       "      <td>25225</td>\n",
       "      <td>101.9%</td>\n",
       "      <td>1/143</td>\n",
       "      <td>1/261</td>\n",
       "      <td>1/317</td>\n",
       "      <td>317.0</td>\n",
       "      <td>0.003155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>563</td>\n",
       "      <td>459745</td>\n",
       "      <td>1650</td>\n",
       "      <td>1452</td>\n",
       "      <td>-9012</td>\n",
       "      <td>99.3%</td>\n",
       "      <td>1/148</td>\n",
       "      <td>1/279</td>\n",
       "      <td>1/317</td>\n",
       "      <td>317.0</td>\n",
       "      <td>0.003155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>548</td>\n",
       "      <td>459138</td>\n",
       "      <td>1715</td>\n",
       "      <td>1440</td>\n",
       "      <td>11068</td>\n",
       "      <td>100.8%</td>\n",
       "      <td>1/146</td>\n",
       "      <td>1/268</td>\n",
       "      <td>1/319</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.003135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>593</td>\n",
       "      <td>431704</td>\n",
       "      <td>1618</td>\n",
       "      <td>1352</td>\n",
       "      <td>14195</td>\n",
       "      <td>101.0%</td>\n",
       "      <td>1/145</td>\n",
       "      <td>1/267</td>\n",
       "      <td>1/319</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.003135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     å°ç•ª      Gæ•°    BB    RB     å·®æš      å‡ºç‡     åˆæˆ    BBç‡    RBç‡  \\\n",
       "11  555  552144  2056  1870  39191  102.4%  1/141  1/269  1/295   \n",
       "20  564  444413  1704  1401  25225  101.9%  1/143  1/261  1/317   \n",
       "19  563  459745  1650  1452  -9012   99.3%  1/148  1/279  1/317   \n",
       "4   548  459138  1715  1440  11068  100.8%  1/146  1/268  1/319   \n",
       "28  593  431704  1618  1352  14195  101.0%  1/145  1/267  1/319   \n",
       "\n",
       "    RB_denominator  RB_frequency  \n",
       "11           295.0      0.003390  \n",
       "20           317.0      0.003155  \n",
       "19           317.0      0.003155  \n",
       "4            319.0      0.003135  \n",
       "28           319.0      0.003135  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. CSV ã‚’èª­ã¿è¾¼ã‚€ï¼ˆãƒ‘ã‚¹ã¯è‡ªåˆ†ã®ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰ãˆã¦ãã ã•ã„ï¼‰ ---\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\akafu\\analytics\\aim_machi\\aggregated.csv') # ä¾‹: './data/aggregated.csv'\n",
    "\n",
    "# --- 2. ã€Œ1/xxxã€è¡¨è¨˜ã‚’æ•°å€¤åŒ– ---\n",
    "df['RB_denominator'] = df['RBç‡'].str.extract(r'1/(\\d+)').astype(float)\n",
    "df['RB_frequency']  = 1 / df['RB_denominator']\n",
    "\n",
    "# --- 3. RB_frequency ã§é™é †ã‚½ãƒ¼ãƒˆ ï¼ˆREGç‡ãŒé«˜ã„é †ï¼‰\n",
    "df_sorted = df.sort_values('RB_frequency', ascending=False)\n",
    "\n",
    "# --- è¡¨ç¤ºã€€---\n",
    "df_sorted.head()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec1fa5c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f5baee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] 2025-06-30 ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: You are trying to merge on int64 and object columns for key 'å°ç•ª'. If you wish to proceed you should use pd.concat\n",
      "[SAVE] C:\\Users\\akafu\\analytics\\aim_machi\\notebook\\result\\filter-next-aim-20250613 20250915.csv (rows=267)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ===== è¨­å®š =====\n",
    "DATA_DIR   = r\"C:\\Users\\akafu\\analytics\\aim_machi\\data2\"                  # èª­ã¿è¾¼ã¿å…ƒï¼ˆæ—¥æ¬¡CSVï¼‰\n",
    "OUTPUT_DIR = r\"C:\\Users\\akafu\\analytics\\aim_machi\\notebook\\result\"       # å‡ºåŠ›å…ˆï¼ˆã¾ã¨ã‚CSVï¼‰\n",
    "START_DATE = \"2025-6-13\"   # â†é–‹å§‹æ—¥\n",
    "END_DATE   = \"2025-9-15\"   # â†çµ‚äº†æ—¥\n",
    "\n",
    "# æŠ½å‡ºæ¡ä»¶\n",
    "G_MIN = 4000\n",
    "RATE_MAX_DEN = 260# REG/RBç‡ã®ã€Œ1/xxxã€ã® xxxï¼ˆåˆ†æ¯ï¼‰ãŒã“ã‚Œä»¥ä¸‹ãªã‚‰OK\n",
    "SAMAI_UPPER = 1000\n",
    "\n",
    "SKIP_MISSING_DAYS = True  # å½“æ—¥ã®CSVãŒç„¡ã„æ—¥ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆFalseã§ã‚¨ãƒ©ãƒ¼ï¼‰\n",
    "\n",
    "# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====\n",
    "def weekday_str(d: datetime) -> str:\n",
    "    return d.strftime(\"%a\")  # Mon/Tue/...\n",
    "\n",
    "def compose_path(d: datetime) -> str:\n",
    "    # aim-YYYY-MM-DD-æ›œ_with_grape.csv ã‚’èª­ã‚€\n",
    "    return os.path.join(\n",
    "        DATA_DIR,\n",
    "        f\"aim-{d.strftime('%Y-%m-%d')}-{weekday_str(d)}_with_grape.csv\"\n",
    "    )\n",
    "\n",
    "def _clean_int_col(df: pd.DataFrame, col: str) -> None:\n",
    "    \"\"\"ã‚«ãƒ³ãƒãƒ»å…¨è§’ãƒã‚¤ãƒŠã‚¹ãƒ»ç©ºç™½ãªã©ã‚’é™¤å»ã—ã¦ã€nullable Int64 ã«å¤‰æ›\"\"\"\n",
    "    if col not in df.columns:\n",
    "        return\n",
    "    s = (df[col].astype(str)\n",
    "                 .str.replace(\"\\u2212\", \"-\", regex=False)   # å…¨è§’ãƒã‚¤ãƒŠã‚¹ â†’ åŠè§’\n",
    "                 .str.replace(\",\", \"\", regex=False)          # ã‚«ãƒ³ãƒé™¤å»\n",
    "                 .str.replace(r\"\\s+\", \"\", regex=True))       # ç©ºç™½é™¤å»\n",
    "    s = s.str.extract(r\"([+-]?\\d+)\", expand=False)           # å…ˆé ­ã®ç¬¦å·ä»˜ãæ•´æ•°ã ã‘æŠ½å‡º\n",
    "    df[col] = pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "def read_day(d: datetime) -> pd.DataFrame:\n",
    "    \"\"\"1æ—¥ã®CSVã‚’èª­ã¿ã€ç‡ã®åˆ†æ¯æŠ½å‡ºï¼‹ä¸»è¦æ•°å€¤åˆ—ã‚’ Int64 æ­£è¦åŒ–ã—ã¦è¿”ã™\"\"\"\n",
    "    path = compose_path(d)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # ç‡ã®åˆ†æ¯ï¼ˆREGç‡å„ªå…ˆã€ç„¡ã‘ã‚Œã°RBç‡ï¼‰ã‚’æ•°å€¤åŒ–\n",
    "    rate_col = \"REGç‡\" if \"REGç‡\" in df.columns else (\"RBç‡\" if \"RBç‡\" in df.columns else None)\n",
    "    if rate_col is None:\n",
    "        raise KeyError(\"REGç‡ ã‚‚ RBç‡ ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "    df[\"rate_denominator\"] = pd.to_numeric(\n",
    "        df[rate_col].astype(str).str.extract(r\"1/(\\d+)\", expand=False),\n",
    "        errors=\"coerce\"\n",
    "    )  # floatã®ã¾ã¾ã§OK\n",
    "\n",
    "    # ä¸»è¦æ•°å€¤åˆ—ã‚’å®‰å…¨ã« Int64 åŒ–\n",
    "    for c in [\"Gæ•°\", \"å·®æš\", \"BB\", \"RB\"]:\n",
    "        _clean_int_col(df, c)\n",
    "\n",
    "    return df\n",
    "\n",
    "def filter_by_conditions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"æŠ½å‡ºæ¡ä»¶ï¼ˆGæ•°ã€rate_denominatorã€å·®æšï¼‰ã§ãƒ•ã‚£ãƒ«ã‚¿\"\"\"\n",
    "    for col in [\"Gæ•°\", \"å·®æš\", \"rate_denominator\"]:\n",
    "        if col not in df.columns:\n",
    "            raise KeyError(f\"åˆ— {col} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "\n",
    "    cond_g    = (df[\"Gæ•°\"] >= G_MIN).fillna(False)\n",
    "    cond_rate = (df[\"rate_denominator\"] <= RATE_MAX_DEN).fillna(False)\n",
    "    cond_s    = (df[\"å·®æš\"] < SAMAI_UPPER).fillna(False)\n",
    "\n",
    "    out = df.loc[cond_g & cond_rate & cond_s].copy()\n",
    "    # è¦‹ã‚„ã™ãï¼šè‰¯ã„é †ï¼ˆåˆ†æ¯ãŒå°ã•ã„ã»ã©è‰¯ã„ï¼‰â†’ Gæ•°å¤§ãã„é †\n",
    "    out = out.sort_values([\"rate_denominator\", \"Gæ•°\"], ascending=[True, False])\n",
    "    return out\n",
    "\n",
    "def attach_next_day_rows(today_date: datetime) -> pd.DataFrame:\n",
    "    \"\"\"å½“æ—¥ã®æŠ½å‡ºçµæœã«ã€ç¿Œæ—¥ã®åŒå°ç•ªã®ã‚«ãƒ©ãƒ ã‚’ _next ã§æ¨ªä»˜ã‘\"\"\"\n",
    "    df_today = read_day(today_date)\n",
    "    filtered = filter_by_conditions(df_today)\n",
    "\n",
    "    next_date = today_date + timedelta(days=1)\n",
    "\n",
    "    # ç¿Œæ—¥ã®CSVï¼ˆç„¡ã‘ã‚Œã°å°ç•ªã ã‘ã®æ ã‚’ä½œã‚Šã€çµåˆå¾Œã¯NaNã®ã¾ã¾æ®‹ã™ï¼‰\n",
    "    try:\n",
    "        df_next = read_day(next_date)\n",
    "    except FileNotFoundError:\n",
    "        df_next = pd.DataFrame({\"å°ç•ª\": filtered[\"å°ç•ª\"].unique()})\n",
    "\n",
    "    # ç¿Œæ—¥ã‹ã‚‰æ‹¾ã†åˆ—\n",
    "    keep_cols = [\"å°ç•ª\", \"å·®æš\", \"Gæ•°\", \"BB\", \"RB\", \"å‡ºç‡\", \"BBç‡\", \"RBç‡\", \"åˆæˆ\"]\n",
    "    exist_cols = [c for c in keep_cols if c in df_next.columns]\n",
    "    df_next_small = df_next[exist_cols].copy()\n",
    "    df_next_small = df_next_small.rename(columns={c: f\"{c}_next\" for c in exist_cols if c != \"å°ç•ª\"})\n",
    "\n",
    "    merged = filtered.merge(df_next_small, on=\"å°ç•ª\", how=\"left\")\n",
    "    merged.insert(0, \"date\", today_date.strftime(\"%Y-%m-%d\"))\n",
    "    merged.insert(1, \"next_date\", next_date.strftime(\"%Y-%m-%d\"))\n",
    "    return merged\n",
    "\n",
    "def daterange(start_date: datetime, end_date: datetime):\n",
    "    d = start_date\n",
    "    while d <= end_date:\n",
    "        yield d\n",
    "        d += timedelta(days=1)\n",
    "\n",
    "# ===== ãƒ¡ã‚¤ãƒ³ï¼ˆã¾ã¨ã‚ã®ã¿ä¿å­˜ï¼‰ =====\n",
    "def run_range(start_date_str: str, end_date_str: str) -> pd.DataFrame:\n",
    "    start = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    end   = datetime.strptime(end_date_str,   \"%Y-%m-%d\")\n",
    "\n",
    "    all_rows = []\n",
    "    for d in daterange(start, end):\n",
    "        path = compose_path(d)\n",
    "        if not os.path.exists(path):\n",
    "            msg = f\"[SKIP] {os.path.basename(path)} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\"\n",
    "            if SKIP_MISSING_DAYS:\n",
    "                print(msg)\n",
    "                continue\n",
    "            else:\n",
    "                raise FileNotFoundError(msg)\n",
    "        try:\n",
    "            day_result = attach_next_day_rows(d)\n",
    "            all_rows.append(day_result)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {d.date()} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"[INFO] æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹è¡ŒãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total = pd.concat(all_rows, ignore_index=True)\n",
    "\n",
    "    # *_next ã‚’æ•´æ•°ï¼ˆnullable Int64ï¼‰ã«æƒãˆã‚‹ï¼ˆæ¬ æã¯ <NA> ã®ã¾ã¾ï¼‰\n",
    "    for c in [\"å·®æš_next\", \"Gæ•°_next\", \"BB_next\", \"RB_next\"]:\n",
    "        if c in total.columns:\n",
    "            total[c] = pd.to_numeric(total[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # å‡ºåŠ›å…ˆãƒ•ã‚©ãƒ«ãƒ€ï¼ˆnotebook\\resultï¼‰ã«ä¿å­˜\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    out_total = os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        f\"filter-next-aim-{start:%Y%m%d} {end:%Y%m%d}.csv\"\n",
    "    )\n",
    "    total.to_csv(out_total, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[SAVE] {out_total} (rows={len(total)})\")\n",
    "    return total\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _ = run_range(START_DATE, END_DATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfd092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] C:\\Users\\akafu\\analytics\\aim_machi\\notebook\\result\\spec-20250613-20250915.csv (rows=72)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ===== è¨­å®š =====\n",
    "DATA_DIR   = r\"C:\\Users\\akafu\\analytics\\aim_machi\\data\"        # èª­ã¿è¾¼ã¿å…ƒï¼ˆæ—¥æ¬¡CSVï¼‰\n",
    "OUTPUT_DIR = r\"C:\\Users\\akafu\\analytics\\aim_machi\\notebook\\result\"      # å‡ºåŠ›å…ˆï¼ˆã¾ã¨ã‚CSVï¼‰\n",
    "START_DATE = \"2025-06-13\"# â†é–‹å§‹æ—¥\n",
    "END_DATE   = \"2025-09-15\" # â†çµ‚äº†æ—¥\n",
    "\n",
    "# æŠ½å‡ºæ¡ä»¶\n",
    "G_MIN = 0\n",
    "RATE_MAX_DEN = 100000    # REG/RBç‡ã®ã€Œ1/xxxã€ã® xxxï¼ˆåˆ†æ¯ï¼‰ãŒã“ã‚Œä»¥ä¸‹ãªã‚‰OK\n",
    "SAMAI_UPPER = 10000     \n",
    "\n",
    "# ç‰¹å®šã®å°\n",
    "SPECIFIC_DAI_BAN_LIST = [\"593\"] \n",
    "\n",
    "SKIP_MISSING_DAYS = True  # å½“æ—¥ã®CSVãŒç„¡ã„æ—¥ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆFalseã§ã‚¨ãƒ©ãƒ¼ï¼‰\n",
    "\n",
    "# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====\n",
    "def weekday_str(d: datetime) -> str:\n",
    "    return d.strftime(\"%a\")  # Mon/Tue/...\n",
    "\n",
    "def compose_path(d: datetime) -> str:\n",
    "    return os.path.join(DATA_DIR, f\"aim-{d.strftime('%Y-%m-%d')}-{weekday_str(d)}.csv\")\n",
    "\n",
    "def _clean_int_col(df: pd.DataFrame, col: str) -> None:\n",
    "    \"\"\"ã‚«ãƒ³ãƒãƒ»å…¨è§’ãƒã‚¤ãƒŠã‚¹ãƒ»ç©ºç™½ãªã©ã‚’é™¤å»ã—ã¦ã€nullable Int64 ã«å¤‰æ›\"\"\"\n",
    "    if col not in df.columns:\n",
    "        return\n",
    "    s = (df[col].astype(str)\n",
    "                 .str.replace(\"\\u2212\", \"-\", regex=False)   # å…¨è§’ãƒã‚¤ãƒŠã‚¹ â†’ åŠè§’\n",
    "                 .str.replace(\",\", \"\", regex=False)          # ã‚«ãƒ³ãƒé™¤å»\n",
    "                 .str.replace(r\"\\s+\", \"\", regex=True))       # ç©ºç™½é™¤å»\n",
    "    s = s.str.extract(r\"([+-]?\\d+)\", expand=False)          # å…ˆé ­ã®ç¬¦å·ä»˜ãæ•´æ•°ã ã‘æŠ½å‡º\n",
    "    df[col] = pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "def read_day(d: datetime) -> pd.DataFrame:\n",
    "    \"\"\"1æ—¥ã®CSVã‚’èª­ã¿ã€ç‡ã®åˆ†æ¯æŠ½å‡ºï¼‹ä¸»è¦æ•°å€¤åˆ—ã‚’ Int64 æ­£è¦åŒ–ã—ã¦è¿”ã™\"\"\"\n",
    "    path = compose_path(d)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # ç‡ã®åˆ†æ¯ï¼ˆREGç‡å„ªå…ˆã€ç„¡ã‘ã‚Œã°RBç‡ï¼‰ã‚’æ•°å€¤åŒ–\n",
    "    rate_col = \"REGç‡\" if \"REGç‡\" in df.columns else (\"RBç‡\" if \"RBç‡\" in df.columns else None)\n",
    "    if rate_col is None:\n",
    "        raise KeyError(\"REGç‡ ã‚‚ RBç‡ ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "    df[\"rate_denominator\"] = pd.to_numeric(\n",
    "        df[rate_col].astype(str).str.extract(r\"1/(\\d+)\", expand=False),\n",
    "        errors=\"coerce\"\n",
    "    )  # floatã®ã¾ã¾ã§OK\n",
    "\n",
    "    # ä¸»è¦æ•°å€¤åˆ—ã‚’å®‰å…¨ã« Int64 åŒ–\n",
    "    for c in [\"Gæ•°\", \"å·®æš\", \"BB\", \"RB\"]:\n",
    "        _clean_int_col(df, c)\n",
    "\n",
    "    return df\n",
    "\n",
    "def filter_by_conditions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"æŠ½å‡ºæ¡ä»¶ï¼ˆGæ•°ã€rate_denominatorã€å·®æšã€å°ç•ªï¼‰ã§ãƒ•ã‚£ãƒ«ã‚¿\"\"\"\n",
    "    for col in [\"Gæ•°\", \"å·®æš\", \"rate_denominator\", \"å°ç•ª\"]:\n",
    "        if col not in df.columns:\n",
    "            raise KeyError(f\"åˆ— {col} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "\n",
    "    cond_g    = (df[\"Gæ•°\"] >= G_MIN).fillna(False)\n",
    "    cond_rate = (df[\"rate_denominator\"] <= RATE_MAX_DEN).fillna(False)\n",
    "    cond_s    = (df[\"å·®æš\"] < SAMAI_UPPER).fillna(False)\n",
    "\n",
    "    # æ–°ã—ãè¿½åŠ ã—ãŸå°ç•ªã®æ¡ä»¶\n",
    "    cond_dai_ban = df[\"å°ç•ª\"].isin(SPECIFIC_DAI_BAN_LIST).fillna(False)\n",
    "    \n",
    "    # å…¨ã¦ã®æ¡ä»¶ã‚’ANDã§çµåˆ\n",
    "    out = df.loc[cond_g & cond_rate & cond_s & cond_dai_ban].copy()\n",
    "    \n",
    "    # è¦‹ã‚„ã™ãï¼šè‰¯ã„é †ï¼ˆåˆ†æ¯ãŒå°ã•ã„ã»ã©è‰¯ã„ï¼‰â†’ Gæ•°å¤§ãã„é †\n",
    "    out = out.sort_values([\"rate_denominator\", \"Gæ•°\"], ascending=[True, False])\n",
    "    return out\n",
    "\n",
    "def attach_next_day_rows(today_date: datetime) -> pd.DataFrame:\n",
    "    \"\"\"å½“æ—¥ã®æŠ½å‡ºçµæœã«ã€ç¿Œæ—¥ã®åŒå°ç•ªã®ã‚«ãƒ©ãƒ ã‚’ _next ã§æ¨ªä»˜ã‘\"\"\"\n",
    "    df_today = read_day(today_date)\n",
    "    filtered = filter_by_conditions(df_today)\n",
    "\n",
    "    next_date = today_date + timedelta(days=1)\n",
    "\n",
    "    # ç¿Œæ—¥ã®CSVï¼ˆç„¡ã‘ã‚Œã°å°ç•ªã ã‘ã®æ ã‚’ä½œã‚Šã€çµåˆå¾Œã¯NaNã®ã¾ã¾æ®‹ã™ï¼‰\n",
    "    try:\n",
    "        df_next = read_day(next_date)\n",
    "    except FileNotFoundError:\n",
    "        df_next = pd.DataFrame({\"å°ç•ª\": filtered[\"å°ç•ª\"].unique()})\n",
    "\n",
    "    # ç¿Œæ—¥ã‹ã‚‰æ‹¾ã†åˆ—\n",
    "    keep_cols = [\"å°ç•ª\", \"å·®æš\", \"Gæ•°\", \"BB\", \"RB\", \"å‡ºç‡\", \"BBç‡\", \"RBç‡\", \"åˆæˆ\"]\n",
    "    exist_cols = [c for c in keep_cols if c in df_next.columns]\n",
    "    df_next_small = df_next[exist_cols].copy()\n",
    "    df_next_small = df_next_small.rename(columns={c: f\"{c}_next\" for c in exist_cols if c != \"å°ç•ª\"})\n",
    "\n",
    "    merged = filtered.merge(df_next_small, on=\"å°ç•ª\", how=\"left\")\n",
    "    merged.insert(0, \"date\", today_date.strftime(\"%Y-%m-%d\"))\n",
    "    merged.insert(1, \"next_date\", next_date.strftime(\"%Y-%m-%d\"))\n",
    "    return merged\n",
    "\n",
    "def daterange(start_date: datetime, end_date: datetime):\n",
    "    d = start_date\n",
    "    while d <= end_date:\n",
    "        yield d\n",
    "        d += timedelta(days=1)\n",
    "\n",
    "# ===== ãƒ¡ã‚¤ãƒ³ï¼ˆã¾ã¨ã‚ã®ã¿ä¿å­˜ï¼‰ =====\n",
    "def run_range(start_date_str: str, end_date_str: str) -> pd.DataFrame:\n",
    "    start = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    end   = datetime.strptime(end_date_str,   \"%Y-%m-%d\")\n",
    "\n",
    "    all_rows = []\n",
    "    for d in daterange(start, end):\n",
    "        path = compose_path(d)\n",
    "        if not os.path.exists(path):\n",
    "            msg = f\"[SKIP] {os.path.basename(path)} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\"\n",
    "            if SKIP_MISSING_DAYS:\n",
    "                print(msg)\n",
    "                continue\n",
    "            else:\n",
    "                raise FileNotFoundError(msg)\n",
    "        try:\n",
    "            day_result = attach_next_day_rows(d)\n",
    "            all_rows.append(day_result)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {d.date()} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"[INFO] æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹è¡ŒãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total = pd.concat(all_rows, ignore_index=True)\n",
    "\n",
    "    # *_next ã‚’æ•´æ•°ï¼ˆnullable Int64ï¼‰ã«æƒãˆã‚‹ï¼ˆæ¬ æã¯ <NA> ã®ã¾ã¾ï¼‰\n",
    "    for c in [\"å·®æš_next\", \"Gæ•°_next\", \"BB_next\", \"RB_next\"]:\n",
    "        if c in total.columns:\n",
    "            total[c] = pd.to_numeric(total[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # å‡ºåŠ›å…ˆãƒ•ã‚©ãƒ«ãƒ€ï¼ˆnotebook\\resultï¼‰ã«ä¿å­˜\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    out_total = os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        f\"spec-{start:%Y%m%d}-{end:%Y%m%d}.csv\"\n",
    "    )\n",
    "    total.to_csv(out_total, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[SAVE] {out_total} (rows={len(total)})\")\n",
    "    return total\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _ = run_range(START_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0db6a190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 77 ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã€Œå¹³å‡ã€è¡Œã‚’æŠ½å‡ºã—ã¾ã—ãŸï¼\n",
      "                 filename  å°ç•ª   å·®æš    Gæ•°      å‡ºç‡  BB  RB     åˆæˆ    BBç‡    RBç‡\n",
      "0  aim-2025-07-01-Tue.csv  å¹³å‡  164  2982  101.8%  11  10  1/142  1/261  1/311\n",
      "1  aim-2025-07-02-Wed.csv  å¹³å‡ -166  3088   98.2%  11   9  1/155  1/285  1/339\n",
      "2  aim-2025-07-03-Thu.csv  å¹³å‡   -2  3311  100.0%  12  10  1/152  1/269  1/347\n",
      "3  aim-2025-07-04-Fri.csv  å¹³å‡ -159  3644   98.5%  13  10  1/155  1/276  1/356\n",
      "4  aim-2025-07-05-Sat.csv  å¹³å‡   84  5879  100.5%  22  18  1/147  1/267  1/328\n",
      "\n",
      "Saved mean rows to C:\\Users\\akafu\\analytics\\aim_machi\\notebook\\result\\ave.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ã€Œå¹³å‡ã€ãƒ©ãƒ™ãƒ«ã®è¡Œã‚’é›†ã‚ã¦ notebook/result/ave.csv ã«ä¿å­˜\n",
    "\n",
    "def find_dir_up(start: Path, target_name: str) -> Path | None:\n",
    "    \"\"\"start ã‹ã‚‰è¦ªæ–¹å‘ã¸ target_name ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ã™\"\"\"\n",
    "    cur = start.resolve()\n",
    "    while True:\n",
    "        cand = cur / target_name\n",
    "        if cand.is_dir():\n",
    "            return cand\n",
    "        if cur.parent == cur:\n",
    "            return None\n",
    "        cur = cur.parent\n",
    "\n",
    "def main():\n",
    "    # å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆ.py ã§ã‚‚ .ipynb ã§ã‚‚å¯¾å¿œï¼‰\n",
    "    base_dir = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
    "\n",
    "    # ä¸Šæ–¹å‘ã«ãŸã©ã£ã¦ data ã‚’ç‰¹å®šï¼ˆè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° base_dir/dataã€ç„¡ã‘ã‚Œã° base_dirï¼‰\n",
    "    data_dir = find_dir_up(base_dir, \"data\") or (base_dir / \"data\")\n",
    "    if not data_dir.is_dir():\n",
    "        data_dir = base_dir  # æœ€å¾Œã®ä¿é™º\n",
    "\n",
    "    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’æ¨å®šï¼ˆdata ã®ã²ã¨ã¤ä¸Šï¼‰\n",
    "    project_root = data_dir.parent if data_dir.name.lower() == \"data\" else base_dir\n",
    "\n",
    "    # å‡ºåŠ›å…ˆã¯ notebook/resultï¼ˆãªã‘ã‚Œã°ä½œã‚‹ï¼‰ã€‚ç„¡ã‘ã‚Œã° base_dir ã«è½ã¨ã™ã€‚\n",
    "    output_dir = project_root / \"notebook\" / \"result\"\n",
    "    try:\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        output_dir = base_dir\n",
    "\n",
    "    # CSV ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ï¼ˆé›†è¨ˆç³»ã¯é™¤å¤–ï¼‰\n",
    "    files = sorted(\n",
    "        f for f in glob.glob(str(data_dir / \"*.csv\"))\n",
    "        if not Path(f).name.startswith((\"ave\", \"mean_rows\", \"last_row_averages\", \"last_rows\",\n",
    "                                        \"Filtered With Next Range\", \"aggregated\"))\n",
    "    )\n",
    "\n",
    "    extracted = []\n",
    "    for fp in files:\n",
    "        # æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’å„ªå…ˆé †ã§è©¦ã™\n",
    "        try:\n",
    "            df = pd.read_csv(fp, encoding=\"utf-8-sig\")\n",
    "        except Exception:\n",
    "            df = pd.read_csv(fp, encoding=\"cp932\", errors=\"replace\")\n",
    "            \n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # === ğŸ’¡ãƒ—ãƒ­ã®ä¿®æ­£ï¼šã‚ˆã‚Šå …ç‰¢ãªã€Œå¹³å‡ã€æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ ===\n",
    "        # ã€Œå°ç•ªã€åˆ—ãŒã‚ã‚Œã°ãã“ã‚’å„ªå…ˆã—ã¦æ¢ã—ã€ç„¡ã‘ã‚Œã°å…¨åˆ—ã‚’æ¢ã™\n",
    "        target_cols = ['å°ç•ª'] if 'å°ç•ª' in df.columns else df.columns\n",
    "        \n",
    "        mean_rows = pd.DataFrame()\n",
    "        for col in target_cols:\n",
    "            # NaNã‚’å›é¿ã—ã€ç©ºç™½ã‚’é™¤å»ã—ã¦ã‹ã‚‰ã€Œå¹³å‡ã€ã®æ–‡å­—ãŒå«ã¾ã‚Œã‚‹ã‹åˆ¤å®š\n",
    "            mask = df[col].astype(str).str.strip().str.contains(\"å¹³å‡\", na=False)\n",
    "            if mask.any():\n",
    "                mean_rows = df[mask]\n",
    "                break  # è¦‹ã¤ã‹ã£ãŸã‚‰ä»–ã®åˆ—ã¯æ¢ã•ãšã«ãƒ«ãƒ¼ãƒ—çµ‚äº†\n",
    "        \n",
    "        # ãã‚Œã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¸\n",
    "        if mean_rows.empty:\n",
    "            continue\n",
    "        # =================================================\n",
    "\n",
    "        # å…ƒãƒ•ã‚¡ã‚¤ãƒ«åã‚’å…ˆé ­åˆ—ã«ä»˜ä¸\n",
    "        mean_rows = mean_rows.copy()\n",
    "        mean_rows.insert(0, \"filename\", Path(fp).name)\n",
    "        extracted.append(mean_rows)\n",
    "\n",
    "    if not extracted:\n",
    "        print(\"ã€Œå¹³å‡ã€ãƒ©ãƒ™ãƒ«ã®è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "        return\n",
    "\n",
    "    result = pd.concat(extracted, ignore_index=True)\n",
    "    print(f\" {len(extracted)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã€Œå¹³å‡ã€è¡Œã‚’æŠ½å‡ºã—ã¾ã—ãŸï¼\")\n",
    "    print(result.head()) # ç¢ºèªã®ãŸã‚ã«å°‘ã—ã ã‘è¡¨ç¤º\n",
    "\n",
    "    out_fp = output_dir / \"ave.csv\"\n",
    "    result.to_csv(out_fp, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\nSaved mean rows to {out_fp}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
