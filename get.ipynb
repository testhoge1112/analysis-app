{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695bae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 保存完了: .\\aim-2026-02-13-Fri.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime, date\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "UA = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "TAG_URL = \"https://min-repo.com/tag/%E3%83%80%E3%82%A4%E3%82%A8%E3%83%BC%E7%94%BA%E5%8C%97%E7%94%BA%E5%BA%97/\"\n",
    "AIM_PATTERN = re.compile(r\"ネオアイムジャグラーEX\")\n",
    "\n",
    "def _extract_dateid_from_href(href: str) -> str | None:\n",
    "    if not href:\n",
    "        return None\n",
    "    u = urlparse(href)\n",
    "    last = u.path.rstrip(\"/\").split(\"/\")[-1]\n",
    "    if last.isdigit():\n",
    "        return last\n",
    "    qs = parse_qs(u.query)\n",
    "    for k in (\"dateid\", \"date_id\", \"id\", \"p\"):\n",
    "        if k in qs and qs[k]:\n",
    "            return qs[k][0]\n",
    "    m = re.search(r\"(?<!\\d)(\\d{6,})(?!\\d)\", href)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def get_dateids(list_url: str = TAG_URL) -> list[str]:\n",
    "    r = requests.get(list_url, headers=UA, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    ids, seen = [], set()\n",
    "    for tr in soup.select(\"div.table_wrap table tbody tr\"):\n",
    "        a = tr.select_one(\"td:nth-of-type(1) a[href]\")\n",
    "        if not a: \n",
    "            continue\n",
    "        did = _extract_dateid_from_href(a.get(\"href\"))\n",
    "        if did and did not in seen:\n",
    "            ids.append(did)\n",
    "            seen.add(did)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def _to_int(s):\n",
    "\n",
    "    \"\"\"\n",
    "    数値を整数に変換する関数。\n",
    "    サイト側の表記揺れ（全角マイナスやダッシュ等）によるエラーを回避し、\n",
    "    確実にマイナス値を取得できるように処理。\n",
    "    \"\"\"\n",
    "   \n",
    "    if pd.isna(s) or s is None:\n",
    "        return None\n",
    "    \n",
    "   \n",
    "    s_str = str(s).replace(\",\", \"\")\n",
    "    \n",
    "    s_str = re.sub(r\"[－−—–]\", \"-\", s_str)\n",
    "    \n",
    "    try:\n",
    "        return int(s_str)\n",
    "    except ValueError:\n",
    "       \n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_daily_page(dateid: str, base: str = \"https://min-repo.com/\") -> dict:\n",
    "    \"\"\"\n",
    "    1日のまとめページにアクセスし、「日付」と「対象機種のリンク」取得する。\n",
    "    \"\"\"\n",
    "    day_url = urljoin(base, f\"{dateid}/\")\n",
    "    r = requests.get(day_url, headers=UA, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "  \n",
    "    kishu_url = None\n",
    "    for a in soup.select(\"table.kishu a[href]\"):\n",
    "        if AIM_PATTERN.search((a.get_text(strip=True) or \"\")):\n",
    "            kishu_url = urljoin(day_url, a[\"href\"])\n",
    "            break\n",
    "    if not kishu_url:\n",
    "        a_tag = soup.find(\"a\", string=AIM_PATTERN)\n",
    "        if a_tag and a_tag.has_attr(\"href\"):\n",
    "            kishu_url = urljoin(day_url, a_tag[\"href\"])\n",
    "\n",
    "   \n",
    "    report_date = None\n",
    "    time_tag = soup.find(\"time\", class_=\"date\")\n",
    "    if time_tag and time_tag.has_attr(\"datetime\"):\n",
    "        date_str = time_tag[\"datetime\"][:10] # YYYY-MM-DD\n",
    "        try:\n",
    "            report_date = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
    "        except ValueError:\n",
    "            pass\n",
    "            \n",
    "    if not report_date:\n",
    "        m = re.search(r\"date=(\\d{4}-\\d{2}-\\d{2})\", r.text)\n",
    "        if m:\n",
    "            report_date = datetime.strptime(m.group(1), \"%Y-%m-%d\").date()\n",
    "        else:\n",
    "            raise RuntimeError(f\"日付の特定に失敗しました: {day_url}\")\n",
    "\n",
    "    return {\n",
    "        \"date\": report_date,\n",
    "        \"dow\": report_date.strftime(\"%a\"),\n",
    "        \"kishu_url\": kishu_url\n",
    "    }\n",
    "\n",
    "def scrape_kishu_data_table(kishu_url: str) -> pd.DataFrame:\n",
    "    r = requests.get(kishu_url, headers=UA, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "    h = soup.find(lambda t: t.name in (\"h2\",\"h3\") and \"データ一覧\" in t.get_text())\n",
    "    table = h.find_next(\"table\") if h else None\n",
    "\n",
    "    if table is None:\n",
    "        for t in soup.select(\"table\"):\n",
    "            ths = [th.get_text(strip=True) for th in t.select(\"tr th\")]\n",
    "            if {\"台番\",\"差枚\",\"G数\"}.issubset(set(ths)):\n",
    "                table = t; break\n",
    "    if table is None:\n",
    "        raise RuntimeError(\"データ一覧のテーブルが見つかりませんでした。\")\n",
    "\n",
    "    headers = [th.get_text(strip=True) for th in table.select(\"tr\")[0].select(\"th\")]\n",
    "    rows = []\n",
    "    for tr in table.select(\"tr\")[1:]:\n",
    "        tds = tr.find_all(\"td\")\n",
    "        if not tds:\n",
    "            continue\n",
    "        row = {}\n",
    "        for hname, td in zip(headers, tds):\n",
    "            row[hname] = td.get_text(strip=True)\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "   \n",
    "    for c in (\"差枚\",\"G数\",\"BB\",\"RB\"):\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].map(_to_int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def save_aim_table(dateid: str, out_dir: str = \".\") -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    指定した日付のデータをスクレイピングし、必要な列だけを抽出してCSVとして保存する。\n",
    "    \"\"\"\n",
    "    page_info = parse_daily_page(dateid)\n",
    "    \n",
    "    if not page_info[\"kishu_url\"]:\n",
    "        raise RuntimeError(\"対象機種のリンクが見つかりません。\")\n",
    "        \n",
    "    df = scrape_kishu_data_table(page_info[\"kishu_url\"])\n",
    "\n",
    "    \n",
    "    wanted = [\"台番\", \"差枚\", \"G数\", \"出率\", \"BB\", \"RB\", \"合成\", \"BB率\", \"RB率\"]\n",
    "    df = df[[c for c in wanted if c in df.columns]]\n",
    "\n",
    "    fname = f\"aim-{page_info['date']:%Y-%m-%d}-{page_info['dow']}.csv\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    path = os.path.join(out_dir, fname)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        print(f\"[SKIP] 既存データあり: {path}\")\n",
    "        return path\n",
    "\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[OK] 保存完了: {path}\")\n",
    "    return path\n",
    "\n",
    "def main():\n",
    "    ids = get_dateids(TAG_URL)\n",
    "    if not ids:\n",
    "        raise SystemExit(\"dateidが取れませんでした。\")\n",
    "    latest_yesterday = ids[0]  # 0 なら前日分\n",
    "    #保存先ディレクトリを指定\n",
    "    save_aim_table(latest_yesterday, out_dir=\"data\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
